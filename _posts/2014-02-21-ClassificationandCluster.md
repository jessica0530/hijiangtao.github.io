---
date: 2014-02-21
layout: post
title: 数据挖掘分类聚类算法学习摘要
thread: 45
categories: 文档
tags: data
---

这两天没干什么别的事，一直在研究一篇有关数据挖掘分类聚类算法的论文。刚开始时是想通过对论文的研读为之后的基于hadoop平台的分类聚类实验打下基础，但在读的过程中才发现很多很多算法都有待自己去学习。

而就是在这个过程中，越来越发现在计算机这个行业，不管你从事那个具体的小方面，当你想真正做出些事情时，算法一定是必不可少的。准备抽个时间把常用算法巩固一下，同时学习一些有关数据挖掘的常用方法。下面是我从文中摘录的一些关于分类聚类的总结与概括，其中每个列出的算法或者涉及方面在今后的生活中都必将需要自己投入大量的时间深入进行研究。

----

##一、有关数据挖掘

###1.1 数据挖掘相关概念与定义

数据挖掘有**广义和狭义之分**。广义的数据挖掘，指从大量的数据中发现隐藏的、内在的和有用的知识或信息的过程。狭义的数据挖掘，是指知识发现中的一个关键步骤，是一个抽取有用模式或建立模型的重要环节。

**知识发现：**知识发现是识别出存在于数据库中有效的、新颖的、具有潜在价值的乃至最终颗粒剂的模式的非平凡过程。

**两者之间的关系**：知识发现是从数据库中发现知识的全部过程，而数据挖掘则是此全部过程的一个特定的关键步骤。

数据发掘的对象不应只局限于数据库，在现实看来，数据仓库是其最新、最符合的对象。

###1.2 数据挖掘与传统统计学之间的关系

数据挖掘是揭示存在数据里的模式及数据间的关系的学科，它强调对大型数据的处理及数据和知识间的潜在关系。统计学是一门关于数据资料的搜集、整理、分析和推理的科学。数据挖掘和统计分析之间有明显的联系，它们有共同的目标，就是发现数据间隐藏的关系。

过去生硬的去区分两者之间的关系实际意义并不大，但是相较于传统统计分析而言，数据挖掘有下列几项特性：

1. 处理大型数据更有优势，且无须太专业的统计背景去使用数据挖掘的工具；
2. 数据挖掘技术不仅涉及统计学原理，而且包括数据库管理、人工智能、机器学习、模式识别、以及数据可视化等技术。
3. 数据挖掘核心是算法，当然也考虑模型和可解释性问题，但算法及可实现性是第一位的。它所强调的首先是发现，其次才是解释。因而，数据挖掘并不过分依赖于严格的逻辑推理，而是大量采用很多“黑箱”方法和本质上是探索性的方法。
4. 数据挖掘，作为很多学科交叉的结果继承了机器学习的“冒险”态度，比统计学更强调实践性、探索性和灵活性。实际上，与现代科学中常见的“从假设出发演绎推理”的做法相比，数据挖掘更多地是一个归纳过程。

----

##二、R语言介绍

###2.1 R语言简介

R是一种为统计计算和图形显示而设计的语言环境，是贝尔实验室(BeflLaboratories)的RickBeeke、JohnChamberS和AllanWilkS开发的S语言的一种实现，提供了一系列统计和图形显示工具。

R是一组数据操作，计算和图形显示工具的整合包。相对于同类软件其特色在于：

1. 有效的数据处理和保存机制。
2. 拥有一整套数组和矩阵操作运算符。
3. 一系列连贯而又完整的数据分析中间工具。
4. 图形统计可对数据直接进行分析显示，可用于多种图形设备。
5. 一种相当完善、简洁和高效的程序设计语言。它包括条件语句、循环语句、用户自定义的递归函数以及输入输出接口。
6. R是彻底面向对象的统计编程语言。
7. R和其它编程语言、数据库之间有很好的接口。
8. R是自由软件且功能不比任何同类软件差。
9. R具有丰富的网上资源，更为重要一点的是R提供了非常丰富的程序包，除了推荐的标准包外还有很多志愿者贡献的贡献包，可直接利用这些包提高工作效率。

###2.2 R语言与数据挖掘

数据挖掘工具可根据应用领域分为三类：

1. **通用单任务类**。仅支持KDD（KDD：Knowledge Discovery in Data，即知识发现）的数据采掘步骤，并且需要大量的预处理和善后处理工作。主要采用决策树、神经网络、基于例子和规则的方法，发现任务大多属于分类范畴。
2. **通用多任务类**。可执行多个领域的知识发现任务，集成了分类、可视化、聚集、概括等多种策略，如IBM公司的IntelligentMiner和Almaden研究中心开发的QUEST系统，SGI公司开发的Mineset系统，加拿大SimonFraser大学开发的DBMiner系统，SPSS公司的Clementine以及SGI公司的Mineset。
3. **专用领域类**。特定领域的数据挖掘工具针对某个特定领域的问题提供解决方案。在设计算法的时候，充分考虑到数据、需求的特殊性，并作了优化。现有的许多数据采掘系统是专为特定目的开发的，用于专用领域的知识发现，对采掘的数据库有语义要求，挖掘的知识和采用的方法也较单一，有些系统虽然能发现多种形式的知识，但基本器学习、统计分析为主，计算量大。

----

##三、分类分析算法

###3.1 分类的一般步骤

第一步，建立模型，描述预定的数据类集或概念集。通过分析由属性描述的数据库元组来构造模型。

第二步，使用模型进行分类。首先需要评估模型（或分类方法）的预测准确率。

###3.2 数据的预处理

在进行分类前，对数据的预处理可以提高分类预测的准确性、有效性和可伸缩性。分类前一般要进行如下几种数据预处理：

1. **数据清理**：为了消除和减少数据噪声和处理缺失值的数据预处理。虽然大部分的分类算法都会处理噪声和缺失值，但在进行分类对数据的清理可以减少学习时的混乱。
2. **相关性分析**：数据中很多属性可能与分类预测任务不相关或是冗余的。因此在分类前进行相关性分析可以删除学习过程中不相关的或冗余的属性，提高分类预测的效率和准确率。
3. **数据变换**：分类前的数据变换主要有**概念分层和规范化两种**。概念分层就是把连续值属性概化为离散的区间，压缩了原来的训练数据，学习时可以减少输入输出操作。规范化是将给定属性的所有值按比例缩放，使得它们落入较小的指定区间，比如落入[0，1]内，可以防止具有较大初始域的属性相对于具有较小初始域的属性权种过大，该方法常用于神经网络和距离度量方法。

###3.3 分类方法的评估标准

1. **准确率**。指模型正确地预测新的或未见过的数据的类标号的能力，这也是模型的首要能力。如果一个模型的分类准确率小于百分之五十，那么可以认为其结果是无价值的。在其他条件等同的情况下，当然首选准确率高的分类方法。
2. **速度**。指产生和使用模型的时间复杂度。产生模型的试验数据集通常是巨量的，因为一般情况下其数量和分类准确率成正比。如果产生和使用模型的时间过长，将严重影响用户的使用。
3. **稳健性**。指给定噪声数据或具有空缺值的数据，模型正确预测的能力。现实中的数据库通常有噪声，有时还很大。如果一个分类器不善于消除噪声的影响，将严重影响分类准确率。
4. **可伸缩性**。指给定大量数据，有效的构造模型的能力。有些分类器在数据量很小的情况下可以有效的构造模型，随着数据量的增大，其构造模型的能力显著下降，这最终也会影响分类准确率。
5. **可解释性**。指学习模型提供的理解和洞察的层次。

###3.4 基于距离分类方法概述

**基本概念**：假定每个类用类中心来表示，每个元组必须和各个类的中心来比较，从而可以找出最近的类中心，得到确定的类标记，基于距离分类一个元组的复杂性一般是O(n)。

**方法应用之KNN算法**：K最临近方法（K Nearest Neighbors，简称KNN）是实际运用中经常被采用的一种基于距离的分类算法。**KNN算法的基本思想**：假定每个类包含多个训练数据，且每个训练数据都有一个唯一的类别标记，计算每个训练数据到待分类元组的距离，取和待分类元组距离最近的k个训练数据，k个数据中哪个类别的训练数据占多数，则待分类元组就属于哪个类别。

###3.5 基于决策树分类方法

一般来说，决策树的构造主要由两个阶段组成：**第一阶段，生成树阶段**。选取部分受训数据建立决策树，决策树是按广度优先建立直到每个叶节点包括相同的类标记为止。**第二阶段，决策树修剪阶段。**用剩余数据检验决策树，如果所建立的决策树不能正确回答所研究的问题，我们要对决策树进行修剪直到建立一棵正确的决策树。

目前决策树修剪策略有三种:**基于代价复杂度的修剪(Cost--ComplexityPruning)、悲观修剪(PeSSimistiCPruning)和MDL修剪(MinimumDeSCriptionLengthPruoing)**。基于代价复杂度的修剪使用了独立的样本集用于修剪，即与决策树生成过程所使用的样本集不同。在很多情况下，特别是当训练集很小时，更期望将所有的样本既用于决策树的生成也用于决策树的修剪。悲观修剪是Quinlan在1987年提出的，将所有的训练样本都用于决策树的生成与修剪，经验表明，该方法产生的树太大并且有时精度不高，在实际使用过程用的较多的并且效果较好的是MDL修剪。

**方法应用之C4.5算法**：国际上最早，最有影响的决策树方法是Quinlan提出的ID3算法。ID3是一个典型的决策树学习系统，它以信息嫡作为分离目标评价函数，采用自顶向下不可返回的策略，搜出全部空间的一部分，它确保决策树建立最简单，每次所做的测试数据最少。但由于工D3具有偏向于选择属性较多的属性、学习简单的逻辑表达能力较差等缺点。Qu1lan在1993年提出了C4.5算法，它既是工D3算法的后继，也成为以后诸多决策树算法的基础。

C4.5除了拥有ID3算法的功能外，还引入了新的方法和增加了新的功能。例如：

1. 用信息增益比例的概念替代信息增益。
2. 合并具有连续属性的值。
3. 可以处理具有缺少属性值得训练样本。
4. 通过使用不容的修剪技术以避免树的过渡拟合(overfitting)。
5. K交叉验证。

**方法应用之VART算法**

###3.6 基于神经网络分类算法

神经网络建立在有自学习能力的数学模型基础上，可以对复杂的数据进行分析，并完成对脑或其他计算机来说极为复杂的模式抽取及趋势分析。神经网络的典型应用是建立分类模型。神经网络将每一个连接看作一个处理单元(PE)，试图模拟人脑神经元的功能。神经网络从经验中学习，常用于发现一组输入数据和一个结果之间的未知联系。同其它方法一样，神经网络首先检测数据中存在的模式，再对从数据中发现的关系进行概括，然后给出预测结果。神经网络由于能对复杂过程进行预测而受到了特别的关注。处理单元采用一系列数学函数，通过汇总和转换对数据进行处理。一个处理单元的功能有限，但若干个处理单元连接起来形成系统后，就可以创建一个智能模型。处理单元可以许多种不同的方式互连，为了更精确地拟合需要为之建立模型的数据，它们可被反复训练若干次，成百次，甚至上千次。处理单元要和输入输出单元连接起来。在网络训练过程中，需对输入单元和输出单元之间的连接强度(即权值)进行修改。某一个连接强度的提高或减弱根据它对产生某一个结果的重要性进行的。连接强度依赖于在反复训练过程中赋予它的权值。训练过程采用一种称为学习规则的数学方法调节权值。神经网络的训练是根据历史样本数据反复进行的。训练过程中，处理单元对数据进行汇总和转换，它们之间的连接被赋以不同的权值。也就是说，为了对每一个样本的结果变量进行预测，一个网络要尝试各种不同的方案。当输出结果在指定的精度级别上与已知结果吻合，或满足其它的结束准则时，网络的训练就不再进行。

神经网络的最大优点是它能精确地对复杂问题进行预测。在与其它方法进行的精度对比
测试中，神经网络的精确度是比较高的。神经网络方法也有一些缺点：

第一，神经网络虽然在预测方面有用但却难于理解。诚然，早期的神经网络工具的确像被指责的那样，是一种“黑盒子”预测引擎，但当今市场上的新型神经网络工具却有了明显的改进。

第二，神经网络易于受训练过度的影响。如果对具有很强学习功能的神经网络用支持这种功能的少量数据进行训练，开始时正如我们希望的那样，网络学习的是数据中的一般趋势，但此后网络却不断地学习训练数据中非常具体的特征，这不是我们所希望的。这样的网络由于记住了训练数据，缺乏概括能力。

**方法应用之BP算法**

----

##四、聚类分析方法

聚类分析是数据挖掘的一项重要功能，而聚类算法是数据挖掘研究领域中一个非常活跃的研究课题。聚类是把一组对象按照相似性归成若干类别，即“物以类聚”。它的目的是使得属于同一类别的对象之间的距离尽可能的小，而不同类别的对象间的距离尽可能的大。

聚类分析就是使用聚类算法来发现有意义的聚类，它的主要依据是把相似的样本归为一类，而把差异大的样本区分开来，这样所生成的簇是一组数据对象的集合，这些对象与同一个簇中的对象彼此相似，而与其他簇中的对象彼此相异。在许多应用中可以把一个簇中的数据对象当做一个整体来对待。

作为一个数据挖掘的功能，聚类分析能作为一个独立的工具来获得数据分布的情况，观察每个簇的特点，集中对特定的簇做进一步的分析。聚类分析也可以作为其他方法（如特征和分类等）的预处理。

目前文献中存在大量的聚类算法。算法的选择取决于数据的类型、目的和应用。如果聚类分析被用作描述或探查的工具，可以对同样的数据尝试多种算法，以发现数据可能揭示的结果。大体上，按照聚类算法的主要思路可以划分为如下几类:**划分方法(partitioningmethods)、层次方法(hierarehiealmethods)、基于密度的方法(Density一basedMethods)、基于模型的方法(model一basedmcthods)**等。

###4.1 数据挖掘对聚类分析方法的要求

数据挖掘技术的一个突出特点是处理巨大的、复杂的数据集，这对聚类分析技术提出了特殊的挑战，要求算法具有可伸缩性、处理不同类型属性的能力、处理高维数据的能力等。具体地说，数据挖掘对聚类的特殊要求如下：

1. 可伸缩性。许多聚类方法在小于1000个数据对象的小数据集合上工作得很好；但是，一个大规模数据库可能包含几百万个对象，在这样的大数据集合样本上进行聚类可能导致较大偏差。
2. 处理不同类型属性的能力。许多聚类方法只能聚类数值型数据。但是，在数据挖掘领域，数据类型是多样的。
3. 用于决定输入参数的领域知识最少。许多聚类方法在聚类分析中要求用户输入一定的参数，例如希望产生类的数目，而且聚类结果对于输入参数十分敏感。参数通常很难确定，特别是对于包含高维对象的数据集来说，更是如此。要求用户输入参数不仅加重了用户的负担，也使得聚类的质量难以控制。
4. 发现任意形状的聚类。许多聚类方法基于欧氏距离来决定聚类。基于这样的距离度量的算法趋向于发现具有相似此尺度和密度的球状簇。
5. 处理噪声数据的能力。绝大多数的现实世界中的数据库都包含了异常值、缺失值或错误的数据。有些聚类方法对于这样的数据较为敏感，可能导致低质量的聚类结果。
6. 对于输入数据的顺序不敏感。有些聚类方法对于输入数据的顺序是敏感的。例如，同一个数据集合，当以不同的顺序提交给同一个方法时，可能生成差别很大的聚类结果。
7. 高维性。一个数据库或者数据仓库可能包含若干维或者属性。许多聚类方法擅长处理低维的数据，可能只涉及两到三维。在高维空间中聚类数据对象是非常有挑战性的，特别是这样的数据可能非常稀疏，而且高度偏斜。
8. 基于约束的聚类。现实世界中的应用可能需要在各种约束条件下进行聚类。要找到既满足特定的约束，又具有良好聚类特性的数据分组是一项具有挑战性的任务。
9. 可解释性和可用性。用户希望聚类结果是可解释的、可理解的、可用的。也就是说，聚类可能需要和特定的语义解释和应用相联系。

###4.2 划分聚类方法

**实例：K-means算法**

输入：聚类个数k，以及包含n个数据对象的数据库；

输出：满足平方误差准则最小的k个聚类。

处理流程：
1. 从n个数据对象任意k个对象作为初始簇中心。
2. 循环下述流程(3)到(4),直到每个聚类不再发生变化为止。
3. 根据每个簇中对象的均值（中心对象），计算每个对象与这些中心对象的距离，并根据最小距离重新对相应对象进行划分。
4. 重新计算每个（有变化）簇的均值。

###4.3 基于密度聚类方法

绝大多数划分方法基于对象之间的距离进行聚类。这样的方法只能发现球状的或凸形的簇，而在发现任意形状的簇上遇到了困难。在这种情况下提出了基于密度的另一类聚类方法。其主要思想是：只要临近区域的密度(对象或数据点的数目)超过某个阀值，就继续聚类。也就是说对给定类中的每个数据点在一个给定范围的区域中必须至少包含某个数目的点。这样的方法可以用来过滤“噪声”孤立点数据，发现任意形状的簇。

###4.4 基于模型聚类方法

基于模型的聚类方法试图优化给定的数据和某些数学模型之间的适应性。该方法经常是基于数据是根据潜在的概率分布生成的假设。基于模型的聚类方法主要有两类：统计学方法和神经网络方法。

###4.5 模糊聚类方法

对于模糊集来说,一个数据点都是以一定程度属于某个类,也可以同时以不同的程度属于几个类。常用的模糊聚类算法是模糊C平均值FCM(FuZZyC一MeanS)算法,该算法是在传统C均值算法中应用了模糊技术。FCM算法的步骤算法步骤如下:

输入:设定聚类数目C和参数b。

输出:聚类结果

1. 初始化各个聚类中心m；
2. REPEAT:
3. 用当前的聚类中心计算隶属度函数；
4. 用当前的隶属度函数更新计算各类聚类中心；
5. UNTIL各样本隶属度值稳定；

当算法收敛时，就得到了各类的聚类中心和各个样本对于各类的隶属度值从而完成了模糊聚类划分。如果需要，还可将模糊聚类结果进行去模糊化，即用一定的规则把模糊类分划分转化为确定性分类。

----

参考：[UDC]《基于数据挖掘的分类和聚类算法研究及R语言实现》，方匡南，暨南大学，2007.4